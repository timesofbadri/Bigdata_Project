\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\title{Large-Scale Image Pattern Recognition\\
Parallel Machine Learning Implementation}

\author{(According to first name's initial's order in English vocabulary )\\
Haitang Hu, Huizhan Lv, Jian Jin, Tianyi Chen}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Image recognition requires high workload because of its intrinsic complex feature space. When it comes to large amount of pictures, common machine learning algorithm shows obvious limitation in pattern recognition in terms of computation efficiency, resource occupation, etc. Thus techniques developed specifically to deal with large-scale data processing could be expected to have obviously superior performance. In this project we implement parallel SVM based on PCA in large-scale image pattern recognition and verify this scheme works in such kind of task with favourable performance as we have expected. 
\end{abstract}

\section{Background and Motivation}
Pattern recognition concerning image has the mark of complex feature space. Consider a $32\times 32$ color picture, each picture will have 3072 features take RGB scheme into account. For a dataset of 60,000 pictures, it is quite a huge load for the classification task.\\
\\
However the ability to deal with classification in image recognition is of quite significance. For instance, when we  look for images by inputting keyword "car" through a search engine, clearly it should distinguish between distinct types of objects and we don't expect a picture of a boat in the search result.\\
\\
We aim to develop an algorithm which makes classification of large-scale image dataset in an efficient manner, that is to say, gives out result quickly with a high level of accuracy. 
\section{Approach}
\subsection{Preprocessing}
We use PCA for preprocessing. In PCA, the goal is to project the data $\vx$ with dimensionality $D$ onto a space having dimensionality $M <D$ while  maximizing the variance of the projected data.\\
\\
Suppose $S$ is the data covariance matrix defined by\begin{equation}
S=Cov(\vx)
\end{equation}
Let $\{z_i\}$ represent $M$ linear combinations of our original $D$ predictors:
\begin{equation}
z_m=\vw_m^T\vx=\sum_{j=1}^D w_{jm} x_j,~~~~m=1,2,...,M
\end{equation}
We now maximize the projected variance \begin{equation}
Var(z_1)=\vw^T_1S\vw_1
\end{equation}  
Constraint condition is $\vw^T_1\vw_1=1$. Make an unconstrained maximization of
\begin{equation}
w^T_1Sw_1-\lambda_1(w_1^T w_1-1).
\end{equation}
where $\lambda_1$ is a Lagrange multiplier. The optimization result is \begin{equation} 
S\vw_1 = \lambda_1\vw_1
\end{equation}
So\begin{equation}
w_1^TSw_1 = \lambda_1
\end{equation}
which reveals that the variance will be a maximum when we set $\vw_1$ equal to the eigenvector having the largest eigenvalue $\lambda_1$. This eigenvector $\vw_1$ is the first principal component.\\
\\
The second principal component $\vw_2$ should also maximize variance, be of unit length, and be orthogonal to $w_1$. We could find that $\vw_2$ should be the eigenvector of $S$ with the second largest eigenvalue. Similarly, we can show that the other dimensions are given by the eigenvectors with decreasing eigenvalues.\\
\\
Since there are too many features in the task, we use PCA as preprocessing, which reduces 3072 features to 100 features. We projected all the features on the first 100 eigenvectors thus acquiring the data for classification.
\subsection{Processing}
We use parallel SVM with Gaussian kernel for classification, which runs on four seperate machines. The standard SVMs are binary classifiers using a linear decision boundary with disciminant function \begin{equation}
g(\vx)=\vw^T\vx^t+w_0
\end{equation}
of which the decision function is \begin{equation}
y_{new}=\text{sign}(\vw^T\vx_i+w_0)
\end{equation}
In our tasks there is no guarantee that the dataset is linear separable, so we introduce kernel function which allow non-linear dicision boundaries.\\
\\
Kernel is based on transformation of original features, define the basis functions \begin{equation}
\vz =\phi(\vx)~\text{where}~z_j=\phi_j(\vx),~j = 1,...,k
\end{equation}
mapping from the $d$-dimensional $\vx$ space to the $k$-dimensional $\vz$ space where we write the discriminant as\begin{eqnarray}
&g(\vz)=\vw^T\vz\\
&g(\vx)=\vw^T\phi(\vx)=\sum_{j=1}^k w_j\phi_j(\vx)
\end{eqnarray}
The dual in standard SVM problem is now
\begin{equation}
L_d=\sum_i\lambda_i-\frac{1}{2}\sum\limits_i\sum\limits_j \lambda_i\lambda_j y_i y_j \phi(x_i)^T \phi(x_j)
\end{equation}
subject to \begin{equation}
\sum_i \lambda_iy_i=0,~~~~0\leq \lambda_i\leq C,~\forall i
\end{equation}
The idea in kernel machines is to replace the inner product of basis functions, $\phi(x_i)^T\phi(x_j)$, by a kernel function, $K(x_i,x_j)$, thus the kernel function also shows up in the discriminant
\begin{equation}
g(\vx)=\vw^T\phi(\vx)=[\sum_{i=1}^k \lambda_i y_i \phi(\vx_i)^T]\phi(\vx)=\sum_{j=1}^k \lambda_i y_i K(\vx_i,\vx)
\end{equation}
Use this kernel function, we do not need to map it to the new space at all. There are multiple types fo kernels and we use Guassian kernel  \begin{equation}
K(x,x')=exp(-\frac{1}{2}(x-x')^T\Sigma^{-1}(x-x'))
\end{equation}


\section{Framework and Implementaion}
\subsection{Framework-Sparks}
\subsection{Implementations}


\section{Result and evaluation}


\section{Conclution}
Image recognition requires high workload because of its intrinsic complex feature space. When it comes to large amount of pictures, common machine learning algorithm shows obvious limitation in pattern recognition in terms of computation efficiency, resource occupation, etc. Thus techniques developed specifically to deal with large-scale data processing could be expected to have obviously superior performance. In this project we implement parallel SVM based on PCA in large-scale image pattern recognition and verify this scheme works in such kind of task with favourable performance as we have expected. 
\end{document}